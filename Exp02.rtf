{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Times New Roman;}{\f1\fnil\fcharset1 Cambria Math;}}
{\*\generator Riched20 10.0.22000}{\*\mmathPr\mmathFont1\mwrapIndent1440 }\viewkind4\uc1 
\pard\sa200\sl276\slmult1\b\f0\fs28\lang9                                                      TensorFlow\par
\b0 TensorFlow is an open-source software library.\par
TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google\rquote s Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research. \par
TensorFlow is a Google product, which is one of the most famous deep learning tools widely used in the research area of machine learning and deep neural network. It came into the market on 9th November 2015 under the Apache License 2.0. It is built in such a way that it can easily run on multiple CPUs and GPUs as well as on mobile operating systems. It consists of various wrappers in distinct languages such as Java, C++, or Python.\par
TensorFlow provides multiple APIs (Application Programming Interfaces). These can be classified into 2 major categories:\par
1) Low level API - TensorFlow Core is the low level API of TensorFlow.\par
2) High level API - built on top of TensorFlow Core\par
\b Installing TensorFlow\b0\par
import tensorflow as tf\par
\b Features of TensorFlow\b0\par
1) Models can be developed easily: TensorFlow supports high-level APIs, through which Machine Learning models can be built easily using Neural Networks.\par
2) Complex Numeric Computations can be done: As the input dataset is huge, the mathematical computations/calculations can be done easily.\par
3) Consists of Machine Learning APIs: TensorFlow is rich in Machine Learning APIs that are of both low-level and high-level. Stable APIs are available in Python and C. Presently, working on APIs for Java, JavaScript, Julia, Matlab, R, etc.\par
4) Easy deployment and computation using CPU, GPU: TensorFlow supports training and building models on CPU and GPU. Computations can be done on both CPU and GPU and can be compared too.\par
5) Contains pre-trained models and datasets: Google has included many datasets and pre-trained models in TensorFlow. Datasets include mnist, vgg_face2, ImageNet, coco etc.\par
\par
1) The \b module tensorflow.math \b0 provides support for many basic mathematical operations.\par
 \b Function tf.log() [alias tf.math.log] \b0 provides support for the natural logarithmic function in Tensorflow. It expects the input in form of complex numbers as $a+bi$   or floating point numbers. The input type is tensor and if the input contains more than one element, an element-wise logarithm is computed, y=\\log_e x\}$   .\par
Syntax: tf.log(x, name=None) or tf.math.log(x, name=None)\par
Parameters: \par
x: A Tensor of type bfloat16, half, float32, float64, complex64 or complex128. \par
name (optional): The name for the operation.\par
Return type: A Tensor with the same size and type as that of x. \par
\par
2) The module tensorflow.math provides support for many basic logical operations. \par
\b Function tf.logical_and() [alias tf.math.logical_and] provides support for the logical AND function in Tensorflow. \b0 It expects the input of bool type. The input types are tensor and if the tensors contains more than one element, an element-wise logical AND is computed,  $x AND y$ .\par
Syntax: tf.logical_and(x, y, name=None) or tf.math.logical_and(x, y, name=None)\par
Parameters:\par
x: A Tensor of type bool.\par
y: A Tensor of type bool.\par
name (optional): The name for the operation.\par
Return type: A Tensor of bool type with the same size as that of x or y.\par
\par
3) \b The module tensorflow.math provides support for many basic mathematical operations. Function tf.logical_or() [alias tf.math.logical_or] provides support for the logical OR function in Tensorflow.\b0  It expects the input of bool type. The input types are tensor and if the tensors contains more than one element, an element-wise logical OR is computed,  $x OR y$ .\par
Syntax: tf.logical_or(x, y, name=None) or tf.math.logical_or(x, y, name=None)\par
Parameters:\par
x: A Tensor of type bool.\par
y: A Tensor of type bool.\par
name (optional): The name for the operation.\par
\par
4) The module tensorflow.math provides support for many basic logical operations.\par
\b Function tf.logical_xor() [alias tf.math.logical_xor] provides support for the logical XOR function in Tensorflow\b0 . It expects the inputs of bool type. The input types are tensor and if the tensors contains more than one element, an element-wise logical XOR is computed,  $x XOR y = (x \\| \\, y) \\, \\&\\& \\, ^^21(x \\&\\& y)$ .\par
Syntax: tf.logical_xor(x, y, name=None) or tf.math.logical_xor(x, y, name=None)\par
Parameters:\par
x: A Tensor of type bool.\par
y: A Tensor of type bool.\par
name (optional): The name for the operation.\par
Return type: A Tensor of bool type with the same size as that of x or y.\par
\par
5) The module tensorflow.math provides support for many basic logical operations.\par
 \b Function tf.logical_not() [alias tf.math.logical_not or tf.Tensor.__invert__] \b0 provides support for the logical NOT function in Tensorflow. It expects the input of bool type. The input type is tensor and if the input contains more than one element, an element-wise logical NOT is computed,  $ NOT x $ \par
Syntax: tf.logical_not(x, name=None) or tf.math.logical_not(x, name=None) or tf.Tensor.__invert__(x, name=None)\par
Parameters:\par
x: A Tensor of type bool.\par
name (optional): The name for the operation.\par
Return type: A Tensor of bool type with the same size as that of x.\par
\par
\b\fs32                                                Keras\par
\b0\fs28 Keras is an open-source high-level Neural Network library, which is written in Python is capable enough to run on Theano, TensorFlow, or CNTK. It was developed by one of the Google engineers, Francois Chollet. It is made user-friendly, extensible, and modular for facilitating faster experimentation with deep neural networks. It not only supports Convolutional Networks and Recurrent Networks individually but also their combination.\par
It cannot handle low-level computations, so it makes use of the Backend library to resolve it. The backend library act as a high-level API wrapper for the low-level API, which lets it run on TensorFlow, CNTK, or Theano.\par
\b Features of Keras\par
\b0 1)It is a multi backend and supports multi-platform, which helps all the encoders come together for coding.\par
2)Research community present for Keras works amazingly with the production community.\par
3)Easy to grasp all concepts.\par
4)It supports fast prototyping.\par
5)It seamlessly runs on CPU as well as GPU.\par
6)It provides the freedom to design any architecture, which then later is utilized as an API for the project.\par
7)It is really very simple to get started with.\par
\par
Keras is compact, easy to learn, high-level Python library run on top of TensorFlow framework. It is made with focus of understanding deep learning techniques, such as creating layers for neural networks maintaining the concepts of shapes and mathematical details. \par
\par
\b The creation of freamework can be of the following two types \f1\u8722?\f0\par
\b0 1)Sequential API\par
2)Functional API\par
\par
\b Consider the following eight steps to create deep learning model in Keras \f1\u8722?\f0\par
\b0 1)Loading the data\par
2)Preprocess the loaded data\par
3)Definition of model\par
4)Compiling the model\par
5)Fit the specified model\par
6)Evaluate it\par
7)Make the required predictions\par
8)Save the model\lang9  \par
\par
\b Advantages of Keras\par
\b0 1. It is very easy to understand and incorporate the faster deployment of network models.\par
2. It has huge community support in the market as most of the AI companies are keen on using it.\par
3. It supports multi backend, which means you can use any one of them among TensorFlow, CNTK, and Theano with Keras as a backend according to your requirement.\par
4. Since it has an easy deployment, it also holds support for cross-platform. Following are the devices on which Keras can be deployed:\par
1)iOS with CoreML\par
2)Android with TensorFlow Android\par
3)Web browser with .js support\par
4)Cloud engine\par
5)Raspberry pi\par
5. It supports Data parallelism, which means Keras can be trained on multiple GPU's at an instance for speeding up the training time and processing a huge amount of data.\par
\b Keras Installation Steps\par
\b0 Step 1: Create virtual environment\par
\b Windows\par
\b0 Windows user can use the below command,\par
\b py -m venv keras\par
\par
\b0 Step 2: Activate the environment\par
This step will configure python and pip executables in your shell path.\par
\b Windows\par
\b0 Windows users move inside the \ldblquote kerasenv\rdblquote  folder and type the below command,\par
\b .\\env\\Scripts\\activate\par
Step 3: Python libraries\par
\par
\b0 Keras depends on the following python libraries.\par
1)Numpy\par
2)Pandas\par
3)Scikit-learn\par
4)Matplotlib\par
5)Scipy\par
6)Seaborn\par
\par
\b Keras Installation Using Python\par
\b0 pip install keras\par
\par
\b summary method\par
\b0 Prints a string summary of the network.\par
\par
\b get_layer method\par
\b0 Model.get_layer(name=None, index=None)\par
Retrieves a layer based on either its name (unique) or index.\par
If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up).\par
Arguments\par
name: String, name of layer.\par
index: Integer, index of layer.\par
Returns\par
A layer instance.\par
\par
\b add method\b0\par
Sequential.add(layer)\par
Adds a layer instance on top of the layer stack.\par
Arguments\par
layer: layer instance.\par
Raises\par
TypeError: If layer is not a layer instance.\par
ValueError: In case the layer argument does not know its input shape.\par
ValueError: In case the layer argument has multiple output tensors, or is already connected somewhere else (forbidden in Sequential models).\par
\par
\b pop method\par
\b0 Sequential.pop()\par
Removes the last layer in the model.\par
Raises\par
TypeError: if there are no layers in the model.\par
\par
\b compile method\par
\b0 Model.compile(\par
    optimizer="rmsprop",\par
    loss=None,\par
    metrics=None,\par
    loss_weights=None,\par
    weighted_metrics=None,\par
    run_eagerly=None,\par
    steps_per_execution=None,\par
    jit_compile=None,\par
    pss_evaluation_shards=0,\par
    **kwargs\par
)\par
Configures the model for training.\par
\par
\b fit method\par
\b0 Model.fit(\par
    x=None,\par
    y=None,\par
    batch_size=None,\par
    epochs=1,\par
    verbose="auto",\par
    callbacks=None,\par
    validation_split=0.0,\par
    validation_data=None,\par
    shuffle=True,\par
    class_weight=None,\par
    sample_weight=None,\par
    initial_epoch=0,\par
    steps_per_epoch=None,\par
    validation_steps=None,\par
    validation_batch_size=None,\par
    validation_freq=1,\par
    max_queue_size=10,\par
    workers=1,\par
    use_multiprocessing=False,\par
)\par
Trains the model for a fixed number of epochs (dataset iterations).\par
\par
\b evaluate method\par
\b0 Model.evaluate(\par
    x=None,\par
    y=None,\par
    batch_size=None,\par
    verbose="auto",\par
    sample_weight=None,\par
    steps=None,\par
    callbacks=None,\par
    max_queue_size=10,\par
    workers=1,\par
    use_multiprocessing=False,\par
    return_dict=False,\par
    **kwargs\par
)\par
Returns the loss value & metrics values for the model in test mode.\par
\par
Computation is done in batches (see the batch_size arg.) \par
\par
\b predict method\par
\b0 Model.predict(\par
    x,\par
    batch_size=None,\par
    verbose="auto",\par
    steps=None,\par
    callbacks=None,\par
    max_queue_size=10,\par
    workers=1,\par
    use_multiprocessing=False,\par
)\par
Generates output predictions for the input samples.\par
\par
\b train_on_batch method\par
\b0 Model.train_on_batch(\par
    x,\par
    y=None,\par
    sample_weight=None,\par
    class_weight=None,\par
    reset_metrics=True,\par
    return_dict=False,\par
)\par
Runs a single gradient update on a single batch of data.\par
\par
\b test_on_batch method\par
\b0 Model.test_on_batch(\par
    x, y=None, sample_weight=None, reset_metrics=True, return_dict=False\par
)\par
Test the model on a single batch of samples.\par
\par
\b predict_on_batch method\par
\b0 Model.predict_on_batch(x)\par
Returns predictions for a single batch of samples.\par
\par
}
 